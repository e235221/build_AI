{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae07fbd2",
   "metadata": {},
   "source": [
    "### step1：watsonx モデルへのアクセス設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c531f915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 14:38:13,085 - INFO - Client successfully initialized\n",
      "2025-09-10 14:38:14,682 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:38:14,682 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:38:15,285 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "/Users/yamawakidaiki/internship/AGAIN/RAG/lib/python3.12/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:428: LifecycleWarning: Model 'ibm/granite-13b-instruct-v2' is in deprecated state from 2025-06-18 until 2025-10-15. IDs of alternative models: ibm/granite-3-3-8b-instruct. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warn(model_state_warning, category=LifecycleWarning)\n",
      "2025-09-10 14:38:15,285 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "/Users/yamawakidaiki/internship/AGAIN/RAG/lib/python3.12/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:428: LifecycleWarning: Model 'ibm/granite-13b-instruct-v2' is in deprecated state from 2025-06-18 until 2025-10-15. IDs of alternative models: ibm/granite-3-3-8b-instruct. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warn(model_state_warning, category=LifecycleWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "watsonx_api_key = \"0i-_-6pigerNnnRaU8_oiybRZz_UxMQuBHpE_copxSdw\"\n",
    "os.environ[\"WATSONX_APIKEY\"] = watsonx_api_key\n",
    "\n",
    "watsonx_project_id = \"b596c884-f867-4771-afcc-f9fd10dae1a4\"\n",
    "os.environ[\"WATSONX_PROJECT_ID\"] = watsonx_project_id\n",
    "\n",
    "from llama_index.llms.ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames\n",
    "\n",
    "rag_gen_parameters = {\n",
    "    GenTextParamsMetaNames.DECODING_METHOD: \"sample\",\n",
    "    GenTextParamsMetaNames.MIN_NEW_TOKENS: 150,\n",
    "    GenTextParamsMetaNames.TEMPERATURE: 0.5,\n",
    "    GenTextParamsMetaNames.TOP_K: 5,\n",
    "    GenTextParamsMetaNames.TOP_P: 0.7\n",
    "}\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",  # モデル名のスペースを修正\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "    max_new_tokens=512,\n",
    "    params=rag_gen_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb6eba",
   "metadata": {},
   "source": [
    "### step2：イベントループの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddfed40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamawakidaiki/internship/AGAIN/RAG/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "\n",
    "# ---\n",
    "# api_key = os.environ.get(\"WATSONX_APIKEY\") or \"YOUR WATSONX API KEY\"\n",
    "# project_id = os.environ.get(\"WATSONX_PROJECT_ID\") or \"YOUR WATSONX PROJECT ID\"\n",
    "# ---\n",
    "os.environ[\"WATSONX_APIKEY\"] = \"0i-_-6pigerNnnRaU8_oiybRZz_UxMQuBHpE_copxSdw\"\n",
    "os.environ[\"WATSONX_PROJECT_ID\"] = \"b596c884-f867-4771-afcc-f9fd10dae1a4\"\n",
    "\n",
    "api_key = os.environ.get(\"WATSONX_APIKEY\")\n",
    "project_id = os.environ.get(\"WATSONX_PROJECT_ID\")\n",
    "\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ed0bd",
   "metadata": {},
   "source": [
    "### step7：日本語LLMの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babdb4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 14:38:16,842 - INFO - Client successfully initialized\n",
      "2025-09-10 14:38:18,057 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:38:18,057 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:38:18,128 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-09-10 14:38:18,128 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-09-10 14:38:18,488 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_chat%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:38:18,488 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_chat%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:38:18,537 - INFO - Successfully finished Get available chat models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_chat%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-09-10 14:38:18,537 - INFO - Successfully finished Get available chat models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_chat%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-09-10 14:38:18,824 - INFO - Client successfully initialized\n",
      "2025-09-10 14:38:18,824 - INFO - Client successfully initialized\n",
      "2025-09-10 14:38:19,645 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:38:19,645 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:38:19,877 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-09-10 14:38:19,877 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-09-10 14:38:20,169 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_chat%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:38:20,169 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_chat%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:38:20,235 - INFO - Successfully finished Get available chat models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_chat%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-09-10 14:38:20,235 - INFO - Successfully finished Get available chat models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_chat%2C%21lifecycle_withdrawn%3Aand&limit=200'\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    GenTextParamsMetaNames.DECODING_METHOD: \"sample\",\n",
    "    GenTextParamsMetaNames.MIN_NEW_TOKENS: 50,\n",
    "    GenTextParamsMetaNames.MAX_NEW_TOKENS: 1024,\n",
    "    GenTextParamsMetaNames.TEMPERATURE: 0.7,\n",
    "    GenTextParamsMetaNames.TOP_K: 50,\n",
    "    GenTextParamsMetaNames.TOP_P: 0.9,\n",
    "}\n",
    "\n",
    "try:\n",
    "    model = ModelInference(\n",
    "        model_id=\"openai/gpt-oss-120b\",\n",
    "        params=parameters,\n",
    "        credentials={\n",
    "            \"apikey\": os.getenv(\"WATSONX_APIKEY\"),\n",
    "            \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "        },\n",
    "        project_id=os.getenv(\"WATSONX_PROJECT_ID\")\n",
    "    )\n",
    "    \n",
    "    # LangChainのLLMラッパーを作成\n",
    "    # Note: ModelInferenceは直接LangChainのLLMとして使えないため、\n",
    "    # 応答を生成する関数を持つカスタムラッパーを作成します。\n",
    "    class CustomWatsonxLLM(WatsonxLLM):\n",
    "        model_inference: ModelInference\n",
    "\n",
    "        def _call(self, prompt, stop=None, run_manager=None, **kwargs):\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            response = self.model_inference.chat(messages=messages)\n",
    "            return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    llm = CustomWatsonxLLM(\n",
    "        model_inference=model,\n",
    "        model_id=\"openai/gpt-oss-120b\",\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=project_id\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67687d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.mluke.tokenization_mluke.MLukeTokenizer'> OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 14:38:23,203 - INFO - Load pretrained SentenceTransformer: pkshatech/GLuCoSE-base-ja\n"
     ]
    }
   ],
   "source": [
    "# checkpoint\n",
    "# トークナイザが SentencePiece を使って正常ロードできるか確認\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"pkshatech/GLuCoSE-base-ja\")\n",
    "print(type(tok), \"OK\")\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"pkshatech/GLuCoSE-base-ja\"  # 必要なら later に batch サイズ等を調整\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "218f829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 14:38:29,752 - INFO - Client successfully initialized\n",
      "2025-09-10 14:38:30,671 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:38:30,671 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:38:30,715 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-09-10 14:38:30,715 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-09-10 14:38:32,797 - INFO - Load pretrained SentenceTransformer: pkshatech/GLuCoSE-base-ja\n",
      "2025-09-10 14:38:32,797 - INFO - Load pretrained SentenceTransformer: pkshatech/GLuCoSE-base-ja\n",
      "2025-09-10 14:38:38,785 - DEBUG - Building index from IDs objects\n",
      "2025-09-10 14:38:38,785 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "# 以下のコードでは、すでに別のモデルを設定していた場合でも、WatsonxLLM クラスを使って granite-3-2-8b-instruct モデルで 上書き する形になります 。\n",
    "rag_gen_parameters[GenTextParamsMetaNames.MAX_NEW_TOKENS] = 512\n",
    "watsonx_llm = WatsonxLLM(\n",
    "\tmodel_id=\"ibm/granite-3-2-8b-instruct\",\n",
    "\turl=\"https://us-south.ml.cloud.ibm.com\",\n",
    "\tproject_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "\tparams=rag_gen_parameters,\n",
    ")\n",
    "\n",
    "# トマトに関するpdfを読み込ませる。\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "loader = PyMuPDFReader()\n",
    "# pdf_doc_ja = loader.load(file_path=\"./docs/housetomato.pdf\")\n",
    "pdf_doc_ja = loader.load(file_path=\"./docs/housetomato.pdf\")\n",
    "\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "\tpdf_doc_ja, transformations=[splitter],\n",
    "\tembed_model=Settings.embed_model\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 日本語に対応した Embedding モデル に変更\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "\tmodel_name=\"pkshatech/GLuCoSE-base-ja\"\n",
    ")\n",
    "\n",
    "# ここから追加\n",
    "\n",
    "# 新しいドキュメントと設定でインデックスを再構築\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512) # 日本語なのでチャンクサイズを調整\n",
    "index_ja = VectorStoreIndex.from_documents(\n",
    "    pdf_doc_ja, # 日本語のドキュメントを使用\n",
    "    transformations=[splitter],\n",
    "    embed_model=Settings.embed_model\n",
    ")\n",
    "\n",
    "# 新しいインデックスでリトリーバーを再構築\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "vector_retriever_ja = index_ja.as_retriever(similarity_top_k=2)\n",
    "bm25_retriever_ja = BM25Retriever.from_defaults(\n",
    "    docstore=index_ja.docstore,\n",
    "    similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd6ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen_prompt_str = (\n",
    "    \"あなたは、1つの入力クエリに基づいて複数の検索クエリを生成する有能なアシスタントです。\\n\"\n",
    "    \"{num_queries}個の検索クエリを、1行につき1つずつ生成してください。\\n\"\n",
    "    \"以下のクエリに関連する検索クエリを生成してください：\\n\"\n",
    "    \"\\n\"\n",
    "    \"クエリ: {query}\\n\"\n",
    "    \"検索クエリ:\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ba4926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 14:38:38,823 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "Settings.llm = watsonx_llm\n",
    "\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "\tdocstore=index.docstore,\n",
    "\tsimilarity_top_k=2\n",
    ")\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "\t[vector_retriever, bm25_retriever],\n",
    "\tsimilarity_top_k=4,\n",
    "\tnum_queries=4,  # クエリ生成を無効にする場合は1\n",
    "\tmode=\"reciprocal_rerank\",\n",
    "\tuse_async=False,\n",
    "\tverbose=False,\n",
    "\tquery_gen_prompt=query_gen_prompt_str  # クエリ生成プロンプトを上書き\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d36b225",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for RetrievalQA\nretriever\n  Input should be a valid dictionary or instance of BaseRetriever [type=model_type, input_value=<llama_index.core.retriev...r object at 0x31da90b90>, input_type=QueryFusionRetriever]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     12\u001b[39m prompt = PromptTemplate(\n\u001b[32m     13\u001b[39m     template=prompt_template, input_variables=[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m chain_type_kwargs = {\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt}\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m qa = \u001b[43mRetrievalQA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstuff\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_source_documents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchain_type_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchain_type_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/internship/AGAIN/RAG/lib/python3.12/site-packages/langchain/chains/retrieval_qa/base.py:123\u001b[39m, in \u001b[36mBaseRetrievalQA.from_chain_type\u001b[39m\u001b[34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m _chain_type_kwargs = chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    118\u001b[39m combine_documents_chain = load_qa_chain(\n\u001b[32m    119\u001b[39m     llm,\n\u001b[32m    120\u001b[39m     chain_type=chain_type,\n\u001b[32m    121\u001b[39m     **_chain_type_kwargs,\n\u001b[32m    122\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/internship/AGAIN/RAG/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:223\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    222\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/internship/AGAIN/RAG/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:223\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    222\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/internship/AGAIN/RAG/lib/python3.12/site-packages/langchain_core/load/serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/internship/AGAIN/RAG/lib/python3.12/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for RetrievalQA\nretriever\n  Input should be a valid dictionary or instance of BaseRetriever [type=model_type, input_value=<llama_index.core.retriev...r object at 0x31da90b90>, input_type=QueryFusionRetriever]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "以下のコンテキストのみを使用して、最後の質問に答えてください。\n",
    "\n",
    "コンテキスト:\n",
    "{context}\n",
    "\n",
    "質問:\n",
    "{question}\n",
    "\n",
    "回答:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d30b96",
   "metadata": {},
   "source": [
    "#### Gradio を使ったチャットボットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa4df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 14:14:31,909 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:14:31,916 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 14:14:32,334 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:17:50,619 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:17:50,633 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n",
      "2025-09-10 14:17:56,660 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-10 14:17:56,664 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n"
     ]
    }
   ],
   "source": [
    "def chat_function(message, history):\n",
    "    try:\n",
    "        response = qa({\"query\": message})\n",
    "        result = response[\"result\"]\n",
    "        \n",
    "        # ソースドキュメントの情報を追加\n",
    "        source_docs = response.get(\"source_documents\")\n",
    "        if source_docs:\n",
    "            result += \"\\n\\n--- ソース ---\\n\"\n",
    "            for doc in source_docs:\n",
    "                # メタデータからページ番号を取得（存在する場合）\n",
    "                page_num = doc.metadata.get('page', 'N/A')\n",
    "                result += f\"- ページ: {page_num}\\n\"\n",
    "                # コンテンツの最初の100文字を表示\n",
    "                result += f\"  内容: {doc.page_content[:100]}...\\n\"\n",
    "                \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"エラーが発生しました: {str(e)}\"\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_function,\n",
    "    title=\"RAG with gpt-4o\",\n",
    "    description=\"PDFドキュメントに関する質問をしてください。\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\n",
    "        \"トマトの育て方について教えてください。\",\n",
    "        \"IBMの2023年の収益はいくらですか？\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6a14d",
   "metadata": {},
   "source": [
    "# ベクトルデータベース\n",
    "- LlamaIndex のインデックスストレージを使用して，ベクトルをRAMではなくベクトルデータベースに保存するようにする。\n",
    "  - ベクトルデータベースは、**HNSW（Hierarchical Navigable Small World）**のような特殊なインデックスアルゴリズムを使って、類似したベクトルをまとめて保存\n",
    "  - 例：すべての本（ベクトル）を一つの長い棚に並べるのではなく（これはブルートフォース検索）、ベクトルデータベースは階層的なシステムを使用する。本をジャンル分け（類似ベクトルごとにカテゴリ作成）=>ジャンルの中でサブジャンルや著者ごとに整理してネットワーク作成。\n",
    "  - 本同士のつながりが、探している本やその近くにある類似した本まで素早く導いてくれる。これが、ベクトルデータベースが類似した埋め込み（エンベディング）を非常に効率的に見つけられる理由"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df88ab0",
   "metadata": {},
   "source": [
    "### step1：DockerのDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b215da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
