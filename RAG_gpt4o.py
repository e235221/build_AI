import os
import gradio as gr
from llama_index.core import VectorStoreIndex, Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.readers.file import PyMuPDFReader
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.llms.ibm import WatsonxLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames

# --------------------------------------------------------------------------
# 1. watsonx ã¨ Embedding ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# --------------------------------------------------------------------------

# watsonxã®APIã‚­ãƒ¼ã¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’è¨­å®šã—ã¾ã™
watsonx_api_key = "0i-_-6pigerNnnRaU8_oiybRZz_UxMQuBHpE_copxSdw"
os.environ["WATSONX_APIKEY"] = watsonx_api_key

watsonx_project_id = "b596c884-f867-4771-afcc-f9fd10dae1a4"
os.environ["WATSONX_PROJECT_ID"] = watsonx_project_id

# LLMã®ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™
rag_gen_parameters = {
    GenTextParamsMetaNames.DECODING_METHOD: "sample",
    GenTextParamsMetaNames.MIN_NEW_TOKENS: 150,
    GenTextParamsMetaNames.MAX_NEW_TOKENS: 512,
    GenTextParamsMetaNames.TEMPERATURE: 0.5,
    GenTextParamsMetaNames.TOP_K: 5,
    GenTextParamsMetaNames.TOP_P: 0.7,
}

# watsonxã®LLMã‚’åˆæœŸåŒ–ã—ã¾ã™
watsonx_llm = WatsonxLLM(
    model_id="openai/gpt-oss-120b",  # ãƒ¢ãƒ‡ãƒ«IDã‚’openai/gpt-oss-120bã«å¤‰æ›´
    url="https://us-south.ml.cloud.ibm.com",
    project_id=os.getenv("WATSONX_PROJECT_ID"),
    params=rag_gen_parameters,
)

# æ—¥æœ¬èªã«å¯¾å¿œã—ãŸEmbeddingãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™
embed_model = HuggingFaceEmbedding(model_name="pkshatech/GLuCoSE-base-ja")

# LlamaIndexå…¨ä½“ã§ä½¿ç”¨ã™ã‚‹LLMã¨Embeddingãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™
Settings.llm = watsonx_llm
Settings.embed_model = embed_model


# --------------------------------------------------------------------------
# 2. PDFãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰
# --------------------------------------------------------------------------

# PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™
loader = PyMuPDFReader()
# PDFãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„
pdf_doc_ja = loader.load(file_path="./docs/housetomato.pdf") 

# ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹ã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼ã‚’å®šç¾©ã—ã¾ã™
splitter = SentenceSplitter(chunk_size=512)

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã™
index = VectorStoreIndex.from_documents(
    pdf_doc_ja,
    transformations=[splitter],
)


# --------------------------------------------------------------------------
# 3. ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã¨ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã®æ§‹ç¯‰
# --------------------------------------------------------------------------

# 2ç¨®é¡ã®ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’æº–å‚™ã—ã¾ã™ (Vector + BM25)
vector_retriever = index.as_retriever(similarity_top_k=2)
bm25_retriever = BM25Retriever.from_defaults(docstore=index.docstore, similarity_top_k=2)

# ã‚¯ã‚¨ãƒªã‚’è¤‡æ•°ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®šç¾©ã—ã¾ã™
query_gen_prompt_str = (
    "ã‚ãªãŸã¯ã€1ã¤ã®å…¥åŠ›ã‚¯ã‚¨ãƒªã«åŸºã¥ã„ã¦è¤‡æ•°ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
    "{num_queries}å€‹ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ã€1è¡Œã«ã¤ã1ã¤ãšã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
    "ä»¥ä¸‹ã®ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š\n\n"
    "ã‚¯ã‚¨ãƒª: {query}\n"
    "æ¤œç´¢ã‚¯ã‚¨ãƒª:\n"
)

# è¤‡æ•°ã®ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’çµ±åˆã™ã‚‹QueryFusionRetrieverã‚’æ§‹ç¯‰ã—ã¾ã™
retriever = QueryFusionRetriever(
    [vector_retriever, bm25_retriever],
    similarity_top_k=4,
    num_queries=4,
    mode="reciprocal_rerank",
    use_async=False,
    verbose=False,
    query_gen_prompt=query_gen_prompt_str,
)

# RAGã®å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™
query_engine = RetrieverQueryEngine(retriever)


# --------------------------------------------------------------------------
# 4. Gradioã«ã‚ˆã‚‹ãƒãƒ£ãƒƒãƒˆUIã®æ§‹ç¯‰ã¨èµ·å‹•
# --------------------------------------------------------------------------

# Gradioã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒå‘¼ã³å‡ºã™é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™
def chat_function(message, history):
    try:
        # query_engine.queryãƒ¡ã‚½ãƒƒãƒ‰ã§å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™
        response_obj = query_engine.query(message)
        result = response_obj.response
        
        # å‚ç…§å…ƒã®æƒ…å ±ã‚’å¿œç­”ã«è¿½åŠ ã—ã¾ã™
        source_nodes = response_obj.source_nodes
        if source_nodes:
            result += "\n\n--- ã‚½ãƒ¼ã‚¹ ---\n"
            # é‡è¤‡ã™ã‚‹ã‚½ãƒ¼ã‚¹ã‚’è¡¨ç¤ºã—ãªã„ã‚ˆã†ã«ç®¡ç†ã—ã¾ã™
            unique_sources = set()
            for node_with_score in source_nodes:
                node = node_with_score.node
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒšãƒ¼ã‚¸ç•ªå·ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—ã—ã¾ã™
                page_num = node.metadata.get('page_label', 'N/A')
                file_name = node.metadata.get('file_name', 'N/A')
                
                source_id = f"{file_name}-p{page_num}"
                if source_id not in unique_sources:
                    result += f"- ãƒ•ã‚¡ã‚¤ãƒ«: {file_name}, ãƒšãƒ¼ã‚¸: {page_num}\n"
                    # å‚ç…§ã•ã‚ŒãŸå†…å®¹ã®å†’é ­éƒ¨åˆ†ã‚’è¡¨ç¤ºã—ã¾ã™
                    content_preview = node.get_content()[:100].replace('\n', ' ')
                    result += f"  å†…å®¹: {content_preview}...\n"
                    unique_sources.add(source_id)
                    
        return result
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

# Gradioã®ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆã—ã¾ã™
demo = gr.ChatInterface(
    fn=chat_function,
    title="ãƒˆãƒãƒˆãƒã‚¹ã‚¿ãƒ¼ğŸ…",
    description="PDFãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«é–¢ã™ã‚‹è³ªå•ã‚’ã—ã¦ãã ã•ã„ã€‚",
    theme="soft",
    examples=[
        "ãƒˆãƒãƒˆã‚’å®¶åº­ã§è‚²ã¦ã‚‹ã«ã¯ã©ã†ã™ã‚Œã°ã‚ˆã„ã§ã™ã‹ï¼Ÿ",
        "ãƒˆãƒãƒˆã®æ ½åŸ¹ã«æœ€é©ãªæ°—å€™ã‚„åœŸå£Œæ¡ä»¶ã¯ä½•ã§ã™ã‹ï¼Ÿ"
    ]
)

# Gradioã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¾ã™
if __name__ == "__main__":
    print("ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’èµ·å‹•ã—ã¾ã™ã€‚URLã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„ã€‚")
    demo.launch()
