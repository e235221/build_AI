{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae07fbd2",
   "metadata": {},
   "source": [
    "### step1：watsonx モデルへのアクセス設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c531f915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:17:57,622 - INFO - Client successfully initialized\n",
      "2025-09-09 16:17:58,621 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:17:59,083 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "/Users/yamawakidaiki/internship/AGAIN/RAG/lib/python3.12/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:428: LifecycleWarning: Model 'ibm/granite-13b-instruct-v2' is in deprecated state from 2025-06-18 until 2025-10-15. IDs of alternative models: ibm/granite-3-3-8b-instruct. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warn(model_state_warning, category=LifecycleWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "watsonx_api_key = \"0i-_-6pigerNnnRaU8_oiybRZz_UxMQuBHpE_copxSdw\"\n",
    "os.environ[\"WATSONX_APIKEY\"] = watsonx_api_key\n",
    "\n",
    "watsonx_project_id = \"b596c884-f867-4771-afcc-f9fd10dae1a4\"\n",
    "os.environ[\"WATSONX_PROJECT_ID\"] = watsonx_project_id\n",
    "\n",
    "from llama_index.llms.ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames\n",
    "\n",
    "rag_gen_parameters = {\n",
    "    GenTextParamsMetaNames.DECODING_METHOD: \"sample\",\n",
    "    GenTextParamsMetaNames.MIN_NEW_TOKENS: 150,\n",
    "    GenTextParamsMetaNames.TEMPERATURE: 0.5,\n",
    "    GenTextParamsMetaNames.TOP_K: 5,\n",
    "    GenTextParamsMetaNames.TOP_P: 0.7\n",
    "}\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",  # モデル名のスペースを修正\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "    max_new_tokens=512,\n",
    "    params=rag_gen_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32f70476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb6eba",
   "metadata": {},
   "source": [
    "### step2：イベントループの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ddfed40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Python の asyncio というライブラリを使って、自分だけの新しいイベントループを作り、両方が問題なく動くようにします。\n",
    "import asyncio, nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "loop = asyncio.get_event_loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5fe1a",
   "metadata": {},
   "source": [
    "### step3：ドキュメント読み込み，埋め込み作成準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "50ead9c1-de56-454e-9ee0-af2c8abd6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 英語バージョン\n",
    "\n",
    "# from pathlib import Path\n",
    "# from llama_index.readers.file import PyMuPDFReader\n",
    "# import requests\n",
    "\n",
    "# def load_pdf(url: str):\n",
    "#     Path(\"docs\").mkdir(exist_ok=True)\n",
    "#     name = url.rsplit(\"/\", 1)[1]\n",
    "#     dst = Path(\"docs\") / name\n",
    "\n",
    "#     r = requests.get(url, timeout=60)\n",
    "#     r.raise_for_status()\n",
    "#     dst.write_bytes(r.content)\n",
    "\n",
    "#     loader = PyMuPDFReader()\n",
    "#     return loader.load(file_path=str(dst))\n",
    "\n",
    "# pdf_doc = load_pdf(\"https://www.ibm.com/annualreport/assets/downloads/IBM_Annual_Report_2023.pdf\")\n",
    "# print(pdf_doc[:1])  # 読み込んだ最初の要素だけ確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1b78397d-a71a-49c3-81d7-cd98342cc317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:17:59,685 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-09-09 16:18:16,855 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSettings.embed_model で使用するモデルを変更できます。 \\n高精度なモデルを使うと意味理解の精度が上がりますが、処理速度\\nやコストが増えます。軽量モデルを使うと高速化やコスト削減ができますが、意味理解の\\n精度が下がる場合があります。\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "\tmodel_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "'''\n",
    "Settings.embed_model で使用するモデルを変更できます。 \n",
    "高精度なモデルを使うと意味理解の精度が上がりますが、処理速度\n",
    "やコストが増えます。軽量モデルを使うと高速化やコスト削減ができますが、意味理解の\n",
    "精度が下がる場合があります。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "34406b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "\tpdf_doc, transformations=[splitter],\n",
    "\tembed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77326ac7",
   "metadata": {},
   "source": [
    "#### 新しいトピックを加える\n",
    "同じトピックに関係する新しいドキュメントを追加したい場合は、既存のインデックスにそのまま挿入できます。\n",
    "まず、前と同じようにファイルを読み込み、適切なパラメータで分割してから、次のように実行します。\n",
    "```\n",
    "nodes = splitter.get_nodes_from_documents(pdf_doc)\n",
    "index.insert_nodes(nodes)\n",
    "```\n",
    "この例ではデータベースは使用していませんが、ストレージを永続化したい場合は、ローカルディスクに次のように保存できます。\n",
    "```\n",
    "index.storage_context.persist(persist_dir=\"./storage\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2a12e",
   "metadata": {},
   "source": [
    "### step4：埋め込み（embeddings）を作成し、ベクトルストアの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "76e68c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:18:21,750 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:18:42,352 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "\tmodel_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56236369",
   "metadata": {},
   "source": [
    "次に、PDF ドキュメントを小さなチャンクに分割し、それぞれをベクトル表現に変換して、**VectorStoreIndex** に保存します。これにより、テキスト検索や質問応答の際に高速で意味的に近い情報を見つけられるようになります。\n",
    "この手順では、**SentenceSplitter** を使って文を 1024 トークン程度のサイズに分割し、それを埋め込みモデルに渡します。\n",
    "パラメータのチューニングによって、検索精度や応答品質を改善することができます：\n",
    "* **chunk_size**\n",
    "    * 1 チャンクあたりに含める最大トークン数を設定します。\n",
    "    * 値を大きくすると文脈が広くなり、より長い情報を 1 つのベクトルに収められますが、検索の粒度が荒くなります。\n",
    "    * 値を小さくすると検索の精度は上がりますが、1 件あたりの情報量が減るため、回答生成に複数チャンクを参照する可能性が高くなります。\n",
    "* **overlap**\n",
    "    * チャンク同士でどれだけテキストを重複させるかを設定できます 。\n",
    "    * 小さなチャンクで切りすぎて文脈が途切れないようにするために、例えば 50〜200 トークン程度を重複させる設定が有効です。\n",
    "* **VectorStoreIndex 設定**\n",
    "    * 保存先（メモリ内・ファイル・クラウド DB など）やインデックス更新方法も調整可能です。\n",
    "    * 大規模データでは永続化ストレージ（faiss、qdrant、 chromadb ）を使うことでスケーラビリティを確保できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7417bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "\tpdf_doc, transformations=[splitter],embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df98ba6",
   "metadata": {},
   "source": [
    "同じトピックに関係する新しいドキュメントを追加したい場合は、既存のインデ\n",
    "ックスにそのまま挿入できます。\n",
    "まず、前と同じようにファイルを読み込み、適切なパラメータで分割してから、次\n",
    "のように実行します。\n",
    "nodes = splitter.get_nodes_from_documents(new_doc)\n",
    "index.insert_nodes(nodes)\n",
    "この例ではデータベースは使用していませんが、ストレージを永続化したい場合\n",
    "は、ローカルディスクに次のように保存できます。\n",
    "index.storage_context.persist(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c26776",
   "metadata": {},
   "source": [
    "### step5：リトリーバーの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "203af2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### クエリを作るためのプロンプトを用意\n",
    "\n",
    "query_gen_prompt_str = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a single input query. \"\n",
    "    \"Generate {num_queries} search queries, one on each line \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682603d",
   "metadata": {},
   "source": [
    "次に、QueryFusionRetriever を使ってクエリを書き換えます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4530850",
   "metadata": {},
   "source": [
    "---\n",
    "このモジュールは、ユーザーのクエリに似た複数のクエリを生成し、それぞれのクエリ（元のクエリも含む）から上位の結果を取得し、Reciprocal Rerank Fusionというアルゴリズムで再評価（リランキング）します。\n",
    "この方法は、余計な計算や外部モデルに頼らずに、取得したクエリと関連する結果を効率よく統合できる手法で、論文でも紹介されています"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ee614",
   "metadata": {},
   "source": [
    "**QueryFusionRetriever** にはいくつかの調整可能なパラメータがあり、これらをチューニングすることで検索精度や速度を最適化できます：\n",
    "* **similarity_top_k**\n",
    "    * 各サブリトリーバー（ベクトル検索、BM25）から取得する上位の結果数。\n",
    "    * 値を大きくすると網羅性が高まりますが、再ランキングの計算量も増えます。\n",
    "    * 小さくすると計算は軽くなりますが、見落としの可能性が高まります。\n",
    "* **num_queries**\n",
    "    * ユーザーのクエリから生成する追加クエリ数。\n",
    "    * 1 にするとクエリ生成を無効化し、元のクエリのみを使用します。\n",
    "    * 値を増やすと異なる視点からの検索ができ、リコール率が向上しますが、リクエスト数と処理時間も増えます。\n",
    "* **mode**\n",
    "    * 複数の結果を統合する方法。\n",
    "    * \"reciprocal_rerank\"では、各リトリーバーの順位を逆数化してスコア化し、それを合算して最終順位を決定します。\n",
    "    * 他の統合モードを使うことで異なるランキング戦略も可能です。\n",
    "* **use_async**\n",
    "    * True にすると非同期処理で並列に検索し、速度が向上します。\n",
    "    * 同期処理（False）は順次実行するため、デバッグや順序依存のケースで有効です。\n",
    "* **verbose**\n",
    "    * True にすると内部処理や中間結果をログ出力し、挙動を確認しやすくなります。\n",
    "* **query_gen_prompt**\n",
    "    * 追加クエリを生成する際のプロンプト。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3b4e9dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yamawakidaiki/internship/AGAIN/RAG/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(11822) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-retrievers-bm25 in ./RAG/lib/python3.12/site-packages (0.6.5)\n",
      "Requirement already satisfied: llama-index in ./RAG/lib/python3.12/site-packages (0.14.0)\n",
      "Requirement already satisfied: llama-index-llms-ibm in ./RAG/lib/python3.12/site-packages (0.6.1)\n",
      "Requirement already satisfied: ibm-watsonx-ai in ./RAG/lib/python3.12/site-packages (1.3.37)\n",
      "Requirement already satisfied: bm25s>=0.2.7.post1 in ./RAG/lib/python3.12/site-packages (from llama-index-retrievers-bm25) (0.2.14)\n",
      "Requirement already satisfied: llama-index-core<0.15,>=0.13.1 in ./RAG/lib/python3.12/site-packages (from llama-index-retrievers-bm25) (0.13.6)\n",
      "Requirement already satisfied: pystemmer<3,>=2.2.0.1 in ./RAG/lib/python3.12/site-packages (from llama-index-retrievers-bm25) (2.2.0.3)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2025.9.0)\n",
      "Requirement already satisfied: httpx in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.9.1)\n",
      "Requirement already satisfied: numpy in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.3.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.4.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in ./RAG/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./RAG/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./RAG/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./RAG/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./RAG/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./RAG/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./RAG/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./RAG/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./RAG/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.20.1)\n",
      "Requirement already satisfied: griffe in ./RAG/lib/python3.12/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in ./RAG/lib/python3.12/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in ./RAG/lib/python3.12/site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./RAG/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.10)\n",
      "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in ./RAG/lib/python3.12/site-packages (from llama-index) (0.5.0)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in ./RAG/lib/python3.12/site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in ./RAG/lib/python3.12/site-packages (from llama-index) (0.9.4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.6,>=0.5.0 in ./RAG/lib/python3.12/site-packages (from llama-index) (0.5.6)\n",
      "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in ./RAG/lib/python3.12/site-packages (from llama-index) (0.5.4)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in ./RAG/lib/python3.12/site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: openai>=1.1.0 in ./RAG/lib/python3.12/site-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.107.0)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in ./RAG/lib/python3.12/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.13.5)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in ./RAG/lib/python3.12/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
      "Requirement already satisfied: pandas<2.3.0 in ./RAG/lib/python3.12/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<7,>=5.1.0 in ./RAG/lib/python3.12/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (6.0.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in ./RAG/lib/python3.12/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./RAG/lib/python3.12/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.8)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./RAG/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./RAG/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./RAG/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./RAG/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\n",
      "Requirement already satisfied: certifi in ./RAG/lib/python3.12/site-packages (from httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./RAG/lib/python3.12/site-packages (from httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./RAG/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./RAG/lib/python3.12/site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./RAG/lib/python3.12/site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./RAG/lib/python3.12/site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./RAG/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./RAG/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./RAG/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.1)\n",
      "Requirement already satisfied: pyarrow in ./RAG/lib/python3.12/site-packages (from llama-index-llms-ibm) (21.0.0)\n",
      "Requirement already satisfied: urllib3 in ./RAG/lib/python3.12/site-packages (from ibm-watsonx-ai) (2.5.0)\n",
      "Requirement already satisfied: lomond in ./RAG/lib/python3.12/site-packages (from ibm-watsonx-ai) (0.3.3)\n",
      "Requirement already satisfied: tabulate in ./RAG/lib/python3.12/site-packages (from ibm-watsonx-ai) (0.9.0)\n",
      "Requirement already satisfied: packaging in ./RAG/lib/python3.12/site-packages (from ibm-watsonx-ai) (25.0)\n",
      "Requirement already satisfied: ibm-cos-sdk<2.15.0,>=2.12.0 in ./RAG/lib/python3.12/site-packages (from ibm-watsonx-ai) (2.14.3)\n",
      "Requirement already satisfied: cachetools in ./RAG/lib/python3.12/site-packages (from ibm-watsonx-ai) (6.2.0)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.14.3 in ./RAG/lib/python3.12/site-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai) (2.14.3)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.14.3 in ./RAG/lib/python3.12/site-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai) (2.14.3)\n",
      "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in ./RAG/lib/python3.12/site-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in ./RAG/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./RAG/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.4.3)\n",
      "Requirement already satisfied: scipy in ./RAG/lib/python3.12/site-packages (from bm25s>=0.2.7.post1->llama-index-retrievers-bm25) (1.16.1)\n",
      "Requirement already satisfied: llama-cloud==0.1.35 in ./RAG/lib/python3.12/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.35)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in ./RAG/lib/python3.12/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.54 in ./RAG/lib/python3.12/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: click<9,>=8.1.7 in ./RAG/lib/python3.12/site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (8.2.1)\n",
      "Requirement already satisfied: python-dotenv<2,>=1.0.1 in ./RAG/lib/python3.12/site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.1)\n",
      "Requirement already satisfied: joblib in ./RAG/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./RAG/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2025.9.1)\n",
      "Requirement already satisfied: greenlet>=1 in ./RAG/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./RAG/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./RAG/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.26.1)\n",
      "Requirement already satisfied: colorama>=0.4 in ./RAG/lib/python3.12/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./RAG/lib/python3.12/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.0.2)\n",
      "PY: /Users/yamawakidaiki/internship/AGAIN/RAG/bin/python\n",
      "HAS retrievers ? False\n",
      "<module 'llama_index.retrievers.bm25' from '/Users/yamawakidaiki/internship/AGAIN/RAG/lib/python3.12/site-packages/llama_index/retrievers/bm25/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "print(sys.executable)  # 念のため表示\n",
    "\n",
    "# BM25拡張と必要物をインストール\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-U\",\n",
    "                \"llama-index-retrievers-bm25\",\n",
    "                \"llama-index\",\n",
    "                \"llama-index-llms-ibm\",\n",
    "                \"ibm-watsonx-ai\"], check=True)\n",
    "\n",
    "# カーネル再起動の動作確認\n",
    "import sys, pkgutil, importlib, llama_index\n",
    "print(\"PY:\", sys.executable)\n",
    "\n",
    "# retrievers サブパッケージが見えるか\n",
    "print(\"HAS retrievers ?\",\n",
    "      any(m.name.endswith(\"retrievers\") for m in pkgutil.iter_modules(llama_index.__path__)))\n",
    "\n",
    "# 直接インポート確認\n",
    "print(importlib.import_module(\"llama_index.retrievers.bm25\"))\n",
    "\n",
    "# =============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be285036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:18:51,035 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core import Settings\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "# LLMの設定を行ないます．\n",
    "Settings.llm = watsonx_llm\n",
    "\n",
    "# リトリーバーを取得します．\n",
    "## ベクターリトリーバー\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "## BM25リトリーバー\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore,\n",
    "    similarity_top_k=2\n",
    ")\n",
    "\n",
    "# QueryFusionRetrieverを初期化します．\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=4,\n",
    "    num_queries=4,  # クエリ生成を無効にする場合は1に設定します．\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=False,\n",
    "    verbose=False,\n",
    "    query_gen_prompt=query_gen_prompt_str  # クエリ生成プロンプトを上書きすることができます．\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2ad72",
   "metadata": {},
   "source": [
    "- Windows の場合は、use_async = False に設定する必要があります。\n",
    "  - これは「resource モジュールが Windows で利用できない」というエラーによるもので、Windows の Python でよく知られている非互換の問題です。\n",
    "  - resource モジュールは Linux や macOS などの POSIX 系システム専用で、システムリソースの管理に使われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e451777",
   "metadata": {},
   "source": [
    "次に、元の PDF ドキュメントからの IBM の財務データに関するテストクエリを使って、リトリーバーがどのようにクエリを生成し、ランキングするかを確認しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bdf63f",
   "metadata": {},
   "source": [
    "#### リトリーバーのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ef522727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:18:52,443 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:18:52,450 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.07 :: Management System Segment View\n",
      "($ in millions)\n",
      "For the year ended December 31:\n",
      "Software\n",
      "Consulting \n",
      "...\n",
      "Score: 0.05 :: Reconciliations of IBM as Reported\n",
      "($ in millions)\n",
      "At December 31:\n",
      "2023\n",
      "2022\n",
      "Assets\n",
      "Total reportable...\n",
      "Score: 0.05 :: ($ in millions except per share amounts)\n",
      "For the year ended December 31:\n",
      "Notes\n",
      "2023\n",
      "2022\n",
      "2021\n",
      "Revenu...\n",
      "Score: 0.02 :: Infrastructure\n",
      "Consulting\n",
      "Software\n",
      "We also expanded profit margins by emphasizing high-\n",
      "value offeri...\n"
     ]
    }
   ],
   "source": [
    "nodes_with_scores = retriever.retrieve(\"What was IBMs revenue in 2023?\")\n",
    "# also could store in a pandas dataframe\n",
    "for node in nodes_with_scores:\n",
    "    print(f\"Score: {node.score:.2f} :: {node.text[:100]}...\") #first 100 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d7b4b",
   "metadata": {},
   "source": [
    "### step6：回答生成（RetrieverQueryEngine）\n",
    "これで、生成したクエリに対して実際に回答を作る準備ができました。\n",
    "そのために使うのが RetrieverQueryEngine です。これは、検索と回答の合成をまとめて行うメインのクエリエンジンです。\n",
    "主な３つの構成要素があります：\n",
    "* **retriever**: クエリに基づいてインデックスから関連するドキュメントやノードを取得する役割です。\n",
    "* **node_postprocessors**: 取得したノードを回答生成に使う前にさらに加工・調整するための処理のリストです。\n",
    "* **response_synthesizer**: 加工されたノードをもとに最終的な回答を生成する役割です。\n",
    "このチュートリアルでは、retriever だけを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4c95df8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:18:54,245 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:18:54,310 - INFO - Successfully finished Get next details for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-09-09 16:18:55,006 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:18:55,008 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n",
      "2025-09-09 16:18:55,828 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:18:55,831 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$61.9 billion\n"
     ]
    }
   ],
   "source": [
    "# これでクエリに対して回答を生成できます。\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine(retriever)\n",
    "\n",
    "# 複数のクエリを作成し、それらを評価・統合した後、2つの異なるリトリーバーにクエリを渡します。\n",
    "response = query_engine.query (\"What was IBMs revenue in 2023?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d7e54645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:18:56,652 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:18:56,656 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n",
      "2025-09-09 16:18:57,834 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:18:57,838 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.8 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:18:58,445 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:18:58,447 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n",
      "2025-09-09 16:19:00,740 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:19:00,749 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price of eggs has increased significantly in the last year.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:19:01,459 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:19:01,463 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n",
      "2025-09-09 16:19:06,263 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:19:06,271 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are many ways to hack into a wifi network. One of the most common ways is by using a tool called a “sniffer”. A sniffer is a piece of software that can be used to listen in on wifi traffic and collect information about the devices connected to the network. Another way to hack into a wifi network is by using a tool called a “ brute force attack”. A brute force attack is a type of attack that uses software to try thousands of different passwords in an attempt to guess the correct one. A third way to hack into a wifi network is by using a tool called a “ dictionary attack”. A dictionary attack is a type of attack that uses software to try common words and phrases as passwords. A fourth way to hack into a wifi network is by using a tool called a “ replay attack”. A replay attack is a type of attack that uses software to re-send wifi traffic that has already been sent, in an attempt to trick the network into thinking that the traffic is new. A fifth way to hack into a wifi network is by using a tool called a “ man-in-the-middle attack”. A man-in-the-middle attack is a type of attack that uses software to intercept wifi traffic and then modify it before sending it on to the intended recipient. A sixth way to hack into a wifi network is by using a tool called a “ shoulder surfing attack”. A shoulder surfing attack is a type of attack that uses software to listen in on wifi traffic and collect information about the devices connected to the network.\n"
     ]
    }
   ],
   "source": [
    "# いろいろなクエリを試してみる。\n",
    "print(query_engine.query(\"What was the Operating (non-GAAP) expense-to-revenue ratio in 2023?\"))\n",
    "print(query_engine.query(\"What does the shareholder report say about the price of eggs?\"))\n",
    "print(query_engine.query(\"How do I hack into a wifi network?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227c105",
   "metadata": {},
   "source": [
    "- Granite モデルは、ドキュメントに書かれている内容に忠実に答えるだけでなく、安全で責任ある振る舞いもします。\n",
    "- Granite 3.0 8B Instruct モデルは、有害や不適切な内容を生成させようとする悪意のある入力（敵対的プロンプト）への耐性を高めるよう設計されています。今回の「Wi-Fi ネットワークをハッキングする方法」という質問は、元の資料には含まれていませんでしたが、モデルに組み込まれた安全対策が働きました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ed0bd",
   "metadata": {},
   "source": [
    "### step7：日本語LLMの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "babdb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check\n",
    "# import sys, subprocess  # 依存パッケージ導入\n",
    "# print(sys.executable)   # 確認用\n",
    "# subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"sentencepiece\"], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "67687d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:19:07,635 - INFO - Load pretrained SentenceTransformer: pkshatech/GLuCoSE-base-ja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.mluke.tokenization_mluke.MLukeTokenizer'> OK\n"
     ]
    }
   ],
   "source": [
    "# checkpoint\n",
    "# トークナイザが SentencePiece を使って正常ロードできるか確認\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"pkshatech/GLuCoSE-base-ja\")\n",
    "print(type(tok), \"OK\")\n",
    "\n",
    "# 研究目的の簡潔な説明のみをコード内コメントとして付記する\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"pkshatech/GLuCoSE-base-ja\"  # 必要なら later に batch サイズ等を調整\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cbd6ce44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:19:16,443 - INFO - Client successfully initialized\n",
      "2025-09-09 16:19:17,433 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:19:17,646 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-09-09 16:19:18,159 - INFO - Load pretrained SentenceTransformer: pkshatech/GLuCoSE-base-ja\n",
      "2025-09-09 16:20:26,715 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本語のドキュメントでインデックスとクエリエンジンを更新しました。\n"
     ]
    }
   ],
   "source": [
    "# 以下のコードでは、すでに別のモデルを設定していた場合でも、WatsonxLLM クラスを使って granite-3-2-8b-instruct モデルで 上書き する形になります 。\n",
    "watsonx_llm = WatsonxLLM(\n",
    "\tmodel_id=\"ibm/granite-3-2-8b-instruct\",\n",
    "\turl=\"https://us-south.ml.cloud.ibm.com\",\n",
    "\tproject_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "\tmax_new_tokens=512,\n",
    "\tparams=rag_gen_parameters,\n",
    ")\n",
    "\n",
    "# トマトに関するpdfを読み込ませる。\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "loader = PyMuPDFReader()\n",
    "pdf_doc_ja = loader.load(file_path=\"./docs/housetomato.pdf\")\n",
    "\n",
    "# 日本語に対応した Embedding モデル に変更\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "\tmodel_name=\"pkshatech/GLuCoSE-base-ja\"\n",
    ")\n",
    "\n",
    "# ここから追加\n",
    "\n",
    "# 新しいドキュメントと設定でインデックスを再構築\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512) # 日本語なのでチャンクサイズを調整\n",
    "index_ja = VectorStoreIndex.from_documents(\n",
    "    pdf_doc_ja, # 日本語のドキュメントを使用\n",
    "    transformations=[splitter],\n",
    "    embed_model=Settings.embed_model\n",
    ")\n",
    "\n",
    "# 新しいインデックスでリトリーバーを再構築\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "vector_retriever_ja = index_ja.as_retriever(similarity_top_k=2)\n",
    "bm25_retriever_ja = BM25Retriever.from_defaults(\n",
    "    docstore=index_ja.docstore,\n",
    "    similarity_top_k=2\n",
    ")\n",
    "\n",
    "# 日本語設定でリトリーバーを初期化\n",
    "retriever_ja = QueryFusionRetriever(\n",
    "    [vector_retriever_ja, bm25_retriever_ja],\n",
    "    similarity_top_k=4,\n",
    "    num_queries=4,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=False,\n",
    "    verbose=False,\n",
    "    query_gen_prompt=query_gen_prompt_str # 日本語プロンプト\n",
    ")\n",
    "\n",
    "# クエリエンジンを新しいリトリーバーで再構築\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine(retriever_ja)\n",
    "print(\"日本語のドキュメントでインデックスとクエリエンジンを更新しました。\")\n",
    "\n",
    "# ベクトルを格納するためのインデックス を作成\n",
    "## 今回は システムプロンプトを日本語 に変更し、トマトに関する指示を与えるようにする。\n",
    "## 今回は「トマトの栽培方法」についての知識ベースとして使えるよう設定する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "355c5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_gen_prompt_str = (\n",
    "    \"あなたは、1つの入力クエリに基づいて複数の検索クエリを生成する有能なアシスタントです。\\n\"\n",
    "    \"{num_queries}個の検索クエリを、1行につき1つずつ生成してください。\\n\"\n",
    "    \"以下のクエリに関連する検索クエリを生成してください：\\n\"\n",
    "    \"\\n\"\n",
    "    \"クエリ: {query}\\n\"\n",
    "    \"検索クエリ:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f453ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:20:27,349 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "Settings.llm = watsonx_llm\n",
    "\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "\tdocstore=index.docstore,\n",
    "\tsimilarity_top_k=2\n",
    ")\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "\t[vector_retriever, bm25_retriever],\n",
    "\tsimilarity_top_k=4,\n",
    "\tnum_queries=4,  # クエリ生成を無効にする場合は1\n",
    "\tmode=\"reciprocal_rerank\",\n",
    "\tuse_async=False,\n",
    "\tverbose=False,\n",
    "\tquery_gen_prompt=query_gen_prompt_str  # クエリ生成プロンプトを上書き\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6311313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:20:29,012 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:20:29,051 - INFO - Successfully finished Get next details for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d30b96",
   "metadata": {},
   "source": [
    "#### Gradio を使ったチャットボットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa4df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:20:30,362 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:20:30,708 - INFO - HTTP Request: GET http://127.0.0.1:7863/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:20:30,764 - INFO - HTTP Request: HEAD http://127.0.0.1:7863/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:21:07,177 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-09-09 16:21:08,024 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-09-09 16:21:09,929 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:21:09,935 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n",
      "2025-09-09 16:21:32,178 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:21:32,191 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n"
     ]
    }
   ],
   "source": [
    "# !pip install gradio\n",
    "import gradio as gr\n",
    "\n",
    "def chat_function(message, history):\n",
    "    \"\"\"\n",
    "    チャットメッセージを処理し，応答を生成します．\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # クエリエンジンでメッセージを処理します．\n",
    "        response_obj = query_engine.query(message)\n",
    "        response_text = response_obj.response\n",
    "    except Exception as e:\n",
    "        # エラーが発生した場合，エラーメッセージを返します．\n",
    "        response_text = f\"エラーが発生しました: {e}\"\n",
    "    return response_text\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_function,\n",
    "    title=\"トマトマスター\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\n",
    "        \"トマトを家庭で育てるにはどうすればよいですか？\",\n",
    "        \"トマトの栽培に最適な気候や土壌条件は何ですか？\"\n",
    "    ],\n",
    "    type='messages'\n",
    ")\n",
    "\n",
    "# Gradioアプリケーションを起動します．\n",
    "demo.launch(inline=True, share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6a14d",
   "metadata": {},
   "source": [
    "# ベクトルデータベース\n",
    "- LlamaIndex のインデックスストレージを使用して，ベクトルをRAMではなくベクトルデータベースに保存するようにする。\n",
    "  - ベクトルデータベースは、**HNSW（Hierarchical Navigable Small World）**のような特殊なインデックスアルゴリズムを使って、類似したベクトルをまとめて保存\n",
    "  - 例：すべての本（ベクトル）を一つの長い棚に並べるのではなく（これはブルートフォース検索）、ベクトルデータベースは階層的なシステムを使用する。本をジャンル分け（類似ベクトルごとにカテゴリ作成）=>ジャンルの中でサブジャンルや著者ごとに整理してネットワーク作成。\n",
    "  - 本同士のつながりが、探している本やその近くにある類似した本まで素早く導いてくれる。これが、ベクトルデータベースが類似した埋め込み（エンベディング）を非常に効率的に見つけられる理由"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df88ab0",
   "metadata": {},
   "source": [
    "### step1：DockerのDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b215da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
