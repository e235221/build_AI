{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae07fbd2",
   "metadata": {},
   "source": [
    "### step1：watsonx モデルへのアクセス設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c531f915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:41:44,858 - INFO - Client successfully initialized\n",
      "2025-09-09 16:41:45,895 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:41:46,980 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "/Users/yamawakidaiki/internship/AGAIN/RAG/lib/python3.12/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:428: LifecycleWarning: Model 'ibm/granite-13b-instruct-v2' is in deprecated state from 2025-06-18 until 2025-10-15. IDs of alternative models: ibm/granite-3-3-8b-instruct. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warn(model_state_warning, category=LifecycleWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "watsonx_api_key = \"0i-_-6pigerNnnRaU8_oiybRZz_UxMQuBHpE_copxSdw\"\n",
    "os.environ[\"WATSONX_APIKEY\"] = watsonx_api_key\n",
    "\n",
    "watsonx_project_id = \"b596c884-f867-4771-afcc-f9fd10dae1a4\"\n",
    "os.environ[\"WATSONX_PROJECT_ID\"] = watsonx_project_id\n",
    "\n",
    "from llama_index.llms.ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames\n",
    "\n",
    "rag_gen_parameters = {\n",
    "    GenTextParamsMetaNames.DECODING_METHOD: \"sample\",\n",
    "    GenTextParamsMetaNames.MIN_NEW_TOKENS: 150,\n",
    "    GenTextParamsMetaNames.TEMPERATURE: 0.5,\n",
    "    GenTextParamsMetaNames.TOP_K: 5,\n",
    "    GenTextParamsMetaNames.TOP_P: 0.7\n",
    "}\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",  # モデル名のスペースを修正\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "    max_new_tokens=512,\n",
    "    params=rag_gen_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "32f70476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb6eba",
   "metadata": {},
   "source": [
    "### step2：イベントループの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ddfed40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Python の asyncio というライブラリを使って、自分だけの新しいイベントループを作り、両方が問題なく動くようにします。\n",
    "import asyncio, nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "loop = asyncio.get_event_loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5fe1a",
   "metadata": {},
   "source": [
    "### step3：ドキュメント読み込み，埋め込み作成準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50ead9c1-de56-454e-9ee0-af2c8abd6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 英語バージョン\n",
    "\n",
    "# from pathlib import Path\n",
    "# from llama_index.readers.file import PyMuPDFReader\n",
    "# import requests\n",
    "\n",
    "# def load_pdf(url: str):\n",
    "#     Path(\"docs\").mkdir(exist_ok=True)\n",
    "#     name = url.rsplit(\"/\", 1)[1]\n",
    "#     dst = Path(\"docs\") / name\n",
    "\n",
    "#     r = requests.get(url, timeout=60)\n",
    "#     r.raise_for_status()\n",
    "#     dst.write_bytes(r.content)\n",
    "\n",
    "#     loader = PyMuPDFReader()\n",
    "#     return loader.load(file_path=str(dst))\n",
    "\n",
    "# pdf_doc = load_pdf(\"https://www.ibm.com/annualreport/assets/downloads/IBM_Annual_Report_2023.pdf\")\n",
    "# print(pdf_doc[:1])  # 読み込んだ最初の要素だけ確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b78397d-a71a-49c3-81d7-cd98342cc317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:41:47,033 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-09-09 16:41:51,092 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSettings.embed_model で使用するモデルを変更できます。 \\n高精度なモデルを使うと意味理解の精度が上がりますが、処理速度\\nやコストが増えます。軽量モデルを使うと高速化やコスト削減ができますが、意味理解の\\n精度が下がる場合があります。\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "\tmodel_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "'''\n",
    "Settings.embed_model で使用するモデルを変更できます。 \n",
    "高精度なモデルを使うと意味理解の精度が上がりますが、処理速度\n",
    "やコストが増えます。軽量モデルを使うと高速化やコスト削減ができますが、意味理解の\n",
    "精度が下がる場合があります。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34406b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import VectorStoreIndex\n",
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "# splitter = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "# \tpdf_doc, transformations=[splitter],\n",
    "# \tembed_model=Settings.embed_model\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77326ac7",
   "metadata": {},
   "source": [
    "#### 新しいトピックを加える\n",
    "同じトピックに関係する新しいドキュメントを追加したい場合は、既存のインデックスにそのまま挿入できます。\n",
    "まず、前と同じようにファイルを読み込み、適切なパラメータで分割してから、次のように実行します。\n",
    "```\n",
    "nodes = splitter.get_nodes_from_documents(pdf_doc)\n",
    "index.insert_nodes(nodes)\n",
    "```\n",
    "この例ではデータベースは使用していませんが、ストレージを永続化したい場合は、ローカルディスクに次のように保存できます。\n",
    "```\n",
    "index.storage_context.persist(persist_dir=\"./storage\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2a12e",
   "metadata": {},
   "source": [
    "### step4：埋め込み（embeddings）を作成し、ベクトルストアの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "76e68c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.core import Settings\n",
    "# Settings.embed_model = HuggingFaceEmbedding(\n",
    "# \tmodel_name=\"BAAI/bge-small-en-v1.5\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56236369",
   "metadata": {},
   "source": [
    "次に、PDF ドキュメントを小さなチャンクに分割し、それぞれをベクトル表現に変換して、**VectorStoreIndex** に保存します。これにより、テキスト検索や質問応答の際に高速で意味的に近い情報を見つけられるようになります。\n",
    "この手順では、**SentenceSplitter** を使って文を 1024 トークン程度のサイズに分割し、それを埋め込みモデルに渡します。\n",
    "パラメータのチューニングによって、検索精度や応答品質を改善することができます：\n",
    "* **chunk_size**\n",
    "    * 1 チャンクあたりに含める最大トークン数を設定します。\n",
    "    * 値を大きくすると文脈が広くなり、より長い情報を 1 つのベクトルに収められますが、検索の粒度が荒くなります。\n",
    "    * 値を小さくすると検索の精度は上がりますが、1 件あたりの情報量が減るため、回答生成に複数チャンクを参照する可能性が高くなります。\n",
    "* **overlap**\n",
    "    * チャンク同士でどれだけテキストを重複させるかを設定できます 。\n",
    "    * 小さなチャンクで切りすぎて文脈が途切れないようにするために、例えば 50〜200 トークン程度を重複させる設定が有効です。\n",
    "* **VectorStoreIndex 設定**\n",
    "    * 保存先（メモリ内・ファイル・クラウド DB など）やインデックス更新方法も調整可能です。\n",
    "    * 大規模データでは永続化ストレージ（faiss、qdrant、 chromadb ）を使うことでスケーラビリティを確保できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7417bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import VectorStoreIndex\n",
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "# splitter = SentenceSplitter(chunk_size=1024)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "# \tpdf_doc, transformations=[splitter],embed_model=Settings.embed_model\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df98ba6",
   "metadata": {},
   "source": [
    "同じトピックに関係する新しいドキュメントを追加したい場合は、既存のインデ\n",
    "ックスにそのまま挿入できます。\n",
    "まず、前と同じようにファイルを読み込み、適切なパラメータで分割してから、次\n",
    "のように実行します。\n",
    "nodes = splitter.get_nodes_from_documents(new_doc)\n",
    "index.insert_nodes(nodes)\n",
    "この例ではデータベースは使用していませんが、ストレージを永続化したい場合\n",
    "は、ローカルディスクに次のように保存できます。\n",
    "index.storage_context.persist(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c26776",
   "metadata": {},
   "source": [
    "### step5：リトリーバーの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "203af2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### クエリを作るためのプロンプトを用意\n",
    "\n",
    "# query_gen_prompt_str = (\n",
    "#     \"You are a helpful assistant that generates multiple search queries based on a single input query. \"\n",
    "#     \"Generate {num_queries} search queries, one on each line \"\n",
    "#     \"related to the following input query:\\n\"\n",
    "#     \"Query: {query}\\n\"\n",
    "#     \"Queries:\\n\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682603d",
   "metadata": {},
   "source": [
    "次に、QueryFusionRetriever を使ってクエリを書き換えます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4530850",
   "metadata": {},
   "source": [
    "---\n",
    "このモジュールは、ユーザーのクエリに似た複数のクエリを生成し、それぞれのクエリ（元のクエリも含む）から上位の結果を取得し、Reciprocal Rerank Fusionというアルゴリズムで再評価（リランキング）します。\n",
    "この方法は、余計な計算や外部モデルに頼らずに、取得したクエリと関連する結果を効率よく統合できる手法で、論文でも紹介されています"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ee614",
   "metadata": {},
   "source": [
    "**QueryFusionRetriever** にはいくつかの調整可能なパラメータがあり、これらをチューニングすることで検索精度や速度を最適化できます：\n",
    "* **similarity_top_k**\n",
    "    * 各サブリトリーバー（ベクトル検索、BM25）から取得する上位の結果数。\n",
    "    * 値を大きくすると網羅性が高まりますが、再ランキングの計算量も増えます。\n",
    "    * 小さくすると計算は軽くなりますが、見落としの可能性が高まります。\n",
    "* **num_queries**\n",
    "    * ユーザーのクエリから生成する追加クエリ数。\n",
    "    * 1 にするとクエリ生成を無効化し、元のクエリのみを使用します。\n",
    "    * 値を増やすと異なる視点からの検索ができ、リコール率が向上しますが、リクエスト数と処理時間も増えます。\n",
    "* **mode**\n",
    "    * 複数の結果を統合する方法。\n",
    "    * \"reciprocal_rerank\"では、各リトリーバーの順位を逆数化してスコア化し、それを合算して最終順位を決定します。\n",
    "    * 他の統合モードを使うことで異なるランキング戦略も可能です。\n",
    "* **use_async**\n",
    "    * True にすると非同期処理で並列に検索し、速度が向上します。\n",
    "    * 同期処理（False）は順次実行するため、デバッグや順序依存のケースで有効です。\n",
    "* **verbose**\n",
    "    * True にすると内部処理や中間結果をログ出力し、挙動を確認しやすくなります。\n",
    "* **query_gen_prompt**\n",
    "    * 追加クエリを生成する際のプロンプト。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b4e9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, subprocess\n",
    "# print(sys.executable)  # 念のため表示\n",
    "\n",
    "# # BM25拡張と必要物をインストール\n",
    "# subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-U\",\n",
    "#                 \"llama-index-retrievers-bm25\",\n",
    "#                 \"llama-index\",\n",
    "#                 \"llama-index-llms-ibm\",\n",
    "#                 \"ibm-watsonx-ai\"], check=True)\n",
    "\n",
    "# # カーネル再起動の動作確認\n",
    "# import sys, pkgutil, importlib, llama_index\n",
    "# print(\"PY:\", sys.executable)\n",
    "\n",
    "# # retrievers サブパッケージが見えるか\n",
    "# print(\"HAS retrievers ?\",\n",
    "#       any(m.name.endswith(\"retrievers\") for m in pkgutil.iter_modules(llama_index.__path__)))\n",
    "\n",
    "# # 直接インポート確認\n",
    "# print(importlib.import_module(\"llama_index.retrievers.bm25\"))\n",
    "\n",
    "# # =============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be285036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.retrievers import QueryFusionRetriever\n",
    "# from llama_index.core import Settings\n",
    "# from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "# # LLMの設定を行ないます．\n",
    "# Settings.llm = watsonx_llm\n",
    "\n",
    "# # リトリーバーを取得します．\n",
    "# ## ベクターリトリーバー\n",
    "# vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "# ## BM25リトリーバー\n",
    "# bm25_retriever = BM25Retriever.from_defaults(\n",
    "#     docstore=index.docstore,\n",
    "#     similarity_top_k=2\n",
    "# )\n",
    "\n",
    "# # QueryFusionRetrieverを初期化します．\n",
    "# retriever = QueryFusionRetriever(\n",
    "#     [vector_retriever, bm25_retriever],\n",
    "#     similarity_top_k=4,\n",
    "#     num_queries=4,  # クエリ生成を無効にする場合は1に設定します．\n",
    "#     mode=\"reciprocal_rerank\",\n",
    "#     use_async=False,\n",
    "#     verbose=False,\n",
    "#     query_gen_prompt=query_gen_prompt_str  # クエリ生成プロンプトを上書きすることができます．\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2ad72",
   "metadata": {},
   "source": [
    "- Windows の場合は、use_async = False に設定する必要があります。\n",
    "  - これは「resource モジュールが Windows で利用できない」というエラーによるもので、Windows の Python でよく知られている非互換の問題です。\n",
    "  - resource モジュールは Linux や macOS などの POSIX 系システム専用で、システムリソースの管理に使われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e451777",
   "metadata": {},
   "source": [
    "次に、元の PDF ドキュメントからの IBM の財務データに関するテストクエリを使って、リトリーバーがどのようにクエリを生成し、ランキングするかを確認しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bdf63f",
   "metadata": {},
   "source": [
    "#### リトリーバーのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef522727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes_with_scores = retriever.retrieve(\"What was IBMs revenue in 2023?\")\n",
    "# # also could store in a pandas dataframe\n",
    "# for node in nodes_with_scores:\n",
    "#     print(f\"Score: {node.score:.2f} :: {node.text[:100]}...\") #first 100 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d7b4b",
   "metadata": {},
   "source": [
    "### step6：回答生成（RetrieverQueryEngine）\n",
    "これで、生成したクエリに対して実際に回答を作る準備ができました。\n",
    "そのために使うのが RetrieverQueryEngine です。これは、検索と回答の合成をまとめて行うメインのクエリエンジンです。\n",
    "主な３つの構成要素があります：\n",
    "* **retriever**: クエリに基づいてインデックスから関連するドキュメントやノードを取得する役割です。\n",
    "* **node_postprocessors**: 取得したノードを回答生成に使う前にさらに加工・調整するための処理のリストです。\n",
    "* **response_synthesizer**: 加工されたノードをもとに最終的な回答を生成する役割です。\n",
    "このチュートリアルでは、retriever だけを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4c95df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # これでクエリに対して回答を生成できます。\n",
    "# from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "# query_engine = RetrieverQueryEngine(retriever)\n",
    "\n",
    "# # 複数のクエリを作成し、それらを評価・統合した後、2つの異なるリトリーバーにクエリを渡します。\n",
    "# response = query_engine.query (\"What was IBMs revenue in 2023?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d7e54645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # いろいろなクエリを試してみる。\n",
    "# print(query_engine.query(\"What was the Operating (non-GAAP) expense-to-revenue ratio in 2023?\"))\n",
    "# print(query_engine.query(\"What does the shareholder report say about the price of eggs?\"))\n",
    "# print(query_engine.query(\"How do I hack into a wifi network?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227c105",
   "metadata": {},
   "source": [
    "- Granite モデルは、ドキュメントに書かれている内容に忠実に答えるだけでなく、安全で責任ある振る舞いもします。\n",
    "- Granite 3.0 8B Instruct モデルは、有害や不適切な内容を生成させようとする悪意のある入力（敵対的プロンプト）への耐性を高めるよう設計されています。今回の「Wi-Fi ネットワークをハッキングする方法」という質問は、元の資料には含まれていませんでしたが、モデルに組み込まれた安全対策が働きました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ed0bd",
   "metadata": {},
   "source": [
    "### step7：日本語LLMの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "babdb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check\n",
    "# import sys, subprocess  # 依存パッケージ導入\n",
    "# print(sys.executable)   # 確認用\n",
    "# subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"sentencepiece\"], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67687d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:41:51,763 - INFO - Load pretrained SentenceTransformer: pkshatech/GLuCoSE-base-ja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.mluke.tokenization_mluke.MLukeTokenizer'> OK\n"
     ]
    }
   ],
   "source": [
    "# checkpoint\n",
    "# トークナイザが SentencePiece を使って正常ロードできるか確認\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"pkshatech/GLuCoSE-base-ja\")\n",
    "print(type(tok), \"OK\")\n",
    "\n",
    "# 研究目的の簡潔な説明のみをコード内コメントとして付記する\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"pkshatech/GLuCoSE-base-ja\"  # 必要なら later に batch サイズ等を調整\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cbd6ce44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:41:57,000 - INFO - Client successfully initialized\n",
      "2025-09-09 16:41:58,133 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:41:58,280 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-09-09 16:42:00,142 - INFO - Load pretrained SentenceTransformer: pkshatech/GLuCoSE-base-ja\n",
      "2025-09-09 16:42:07,685 - DEBUG - Building index from IDs objects\n",
      "2025-09-09 16:42:08,516 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:42:08,907 - INFO - Successfully finished Get next details for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本語のドキュメントでインデックスとクエリエンジンを更新しました。\n"
     ]
    }
   ],
   "source": [
    "# 以下のコードでは、すでに別のモデルを設定していた場合でも、WatsonxLLM クラスを使って granite-3-2-8b-instruct モデルで 上書き する形になります 。\n",
    "watsonx_llm = WatsonxLLM(\n",
    "\tmodel_id=\"ibm/granite-3-2-8b-instruct\",\n",
    "\turl=\"https://us-south.ml.cloud.ibm.com\",\n",
    "\tproject_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "\tmax_new_tokens=512,\n",
    "\tparams=rag_gen_parameters,\n",
    ")\n",
    "\n",
    "# トマトに関するpdfを読み込ませる。\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "loader = PyMuPDFReader()\n",
    "pdf_doc_ja = loader.load(file_path=\"./docs/housetomato.pdf\")\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "\tpdf_doc_ja, transformations=[splitter],\n",
    "\tembed_model=Settings.embed_model\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 日本語に対応した Embedding モデル に変更\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "\tmodel_name=\"pkshatech/GLuCoSE-base-ja\"\n",
    ")\n",
    "\n",
    "# ここから追加\n",
    "\n",
    "# 新しいドキュメントと設定でインデックスを再構築\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512) # 日本語なのでチャンクサイズを調整\n",
    "index_ja = VectorStoreIndex.from_documents(\n",
    "    pdf_doc_ja, # 日本語のドキュメントを使用\n",
    "    transformations=[splitter],\n",
    "    embed_model=Settings.embed_model\n",
    ")\n",
    "\n",
    "# 新しいインデックスでリトリーバーを再構築\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "vector_retriever_ja = index_ja.as_retriever(similarity_top_k=2)\n",
    "bm25_retriever_ja = BM25Retriever.from_defaults(\n",
    "    docstore=index_ja.docstore,\n",
    "    similarity_top_k=2\n",
    ")\n",
    "\n",
    "# 日本語設定でリトリーバーを初期化\n",
    "retriever_ja = QueryFusionRetriever(\n",
    "    [vector_retriever_ja, bm25_retriever_ja],\n",
    "    similarity_top_k=4,\n",
    "    num_queries=4,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=False,\n",
    "    verbose=False,\n",
    "    query_gen_prompt=query_gen_prompt_str # 日本語プロンプト\n",
    ")\n",
    "\n",
    "# クエリエンジンを新しいリトリーバーで再構築\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine(retriever_ja)\n",
    "print(\"日本語のドキュメントでインデックスとクエリエンジンを更新しました。\")\n",
    "\n",
    "# ベクトルを格納するためのインデックス を作成\n",
    "## 今回は システムプロンプトを日本語 に変更し、トマトに関する指示を与えるようにする。\n",
    "## 今回は「トマトの栽培方法」についての知識ベースとして使えるよう設定する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "355c5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_gen_prompt_str = (\n",
    "    \"あなたは、1つの入力クエリに基づいて複数の検索クエリを生成する有能なアシスタントです。\\n\"\n",
    "    \"{num_queries}個の検索クエリを、1行につき1つずつ生成してください。\\n\"\n",
    "    \"以下のクエリに関連する検索クエリを生成してください：\\n\"\n",
    "    \"\\n\"\n",
    "    \"クエリ: {query}\\n\"\n",
    "    \"検索クエリ:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7f453ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:42:09,138 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "Settings.llm = watsonx_llm\n",
    "\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "\tdocstore=index.docstore,\n",
    "\tsimilarity_top_k=2\n",
    ")\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "\t[vector_retriever, bm25_retriever],\n",
    "\tsimilarity_top_k=4,\n",
    "\tnum_queries=4,  # クエリ生成を無効にする場合は1\n",
    "\tmode=\"reciprocal_rerank\",\n",
    "\tuse_async=False,\n",
    "\tverbose=False,\n",
    "\tquery_gen_prompt=query_gen_prompt_str  # クエリ生成プロンプトを上書き\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6311313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:42:09,680 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:42:09,878 - INFO - Successfully finished Get next details for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-27&project_id=b596c884-f867-4771-afcc-f9fd10dae1a4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d30b96",
   "metadata": {},
   "source": [
    "#### Gradio を使ったチャットボットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4aaa4df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:42:12,325 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:42:12,337 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:42:12,706 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:42:19,769 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:42:19,772 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n",
      "2025-09-09 16:42:26,925 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27 \"HTTP/1.1 200 OK\"\n",
      "2025-09-09 16:42:26,930 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-08-27'\n"
     ]
    }
   ],
   "source": [
    "# !pip install gradio\n",
    "import gradio as gr\n",
    "\n",
    "def chat_function(message, history):\n",
    "    \"\"\"\n",
    "    チャットメッセージを処理し，応答を生成します．\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # クエリエンジンでメッセージを処理します．\n",
    "        response_obj = query_engine.query(message)\n",
    "        response_text = response_obj.response\n",
    "    except Exception as e:\n",
    "        # エラーが発生した場合，エラーメッセージを返します．\n",
    "        response_text = f\"エラーが発生しました: {e}\"\n",
    "    return response_text\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_function,\n",
    "    title=\"トマトマスター\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\n",
    "        \"トマトを家庭で育てるにはどうすればよいですか？\",\n",
    "        \"トマトの栽培に最適な気候や土壌条件は何ですか？\"\n",
    "    ],\n",
    "    type='messages'\n",
    ")\n",
    "\n",
    "# Gradioアプリケーションを起動します．\n",
    "demo.launch(inline=True, share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6a14d",
   "metadata": {},
   "source": [
    "# ベクトルデータベース\n",
    "- LlamaIndex のインデックスストレージを使用して，ベクトルをRAMではなくベクトルデータベースに保存するようにする。\n",
    "  - ベクトルデータベースは、**HNSW（Hierarchical Navigable Small World）**のような特殊なインデックスアルゴリズムを使って、類似したベクトルをまとめて保存\n",
    "  - 例：すべての本（ベクトル）を一つの長い棚に並べるのではなく（これはブルートフォース検索）、ベクトルデータベースは階層的なシステムを使用する。本をジャンル分け（類似ベクトルごとにカテゴリ作成）=>ジャンルの中でサブジャンルや著者ごとに整理してネットワーク作成。\n",
    "  - 本同士のつながりが、探している本やその近くにある類似した本まで素早く導いてくれる。これが、ベクトルデータベースが類似した埋め込み（エンベディング）を非常に効率的に見つけられる理由"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df88ab0",
   "metadata": {},
   "source": [
    "### step1：DockerのDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b215da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
